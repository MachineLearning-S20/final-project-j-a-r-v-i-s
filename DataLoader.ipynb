{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataLoader.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO9kjEdrijYM",
        "colab_type": "code",
        "outputId": "8fa935f0-aa9e-45f3-984d-c5f310cf2fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision.models.resnet import ResNet, BasicBlock\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from skimage.color import rgb2gray\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import metrics\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "import datetime \n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import io \n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from skimage import img_as_ubyte\n",
        "import os\n",
        "from skimage.util import random_noise"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfWyMm64i6Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ASL_Dataset(Dataset):\n",
        "    def __init__(self, path_2_data, path_2_labels, train=None):\n",
        "      \n",
        "        # Load data from files\n",
        "        data = np.load(path_2_data)\n",
        "        char_labels = np.squeeze(np.load(path_2_labels))\n",
        "        \n",
        "        # Convert char labels to numeric\n",
        "        ulabels = np.unique(char_labels)\n",
        "        num_labels = np.zeros(len(char_labels))\n",
        "        for i in range(len(ulabels)):\n",
        "            num_labels[char_labels == ulabels[i]] = i\n",
        "\n",
        "        ########################################################\n",
        "        X = []\n",
        "        for i in range(data.shape[0]):\n",
        "            img = data[i]\n",
        "            img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
        "            img_g = rgb2gray(img_r)\n",
        "            img_1d = img_g.flatten()\n",
        "            #img_f = img_1d.reshape(1,50,50)\n",
        "            X.append(img_1d)\n",
        "        X = np.asarray(X)\n",
        "        ######################################################## \n",
        "        if (train == True):\n",
        "          data_raw = pd.read_csv(\"drive/My Drive/sign_mnist_train.csv\")\n",
        "          test_data_raw = test = pd.read_csv(\"drive/My Drive/sign_mnist_test.csv\")\n",
        "\n",
        "          # Break up data and labels\n",
        "          labels = data_raw['label']\n",
        "          data_raw.drop('label', axis=1, inplace=True)\n",
        "          labels_test = test_data_raw['label']\n",
        "          test_data_raw.drop('label', axis=1, inplace=True)\n",
        "\n",
        "          # Normalize data\n",
        "          data_full = data_raw.values/255\n",
        "          labels_full = labels.values\n",
        "          test_data_full = test_data_raw.values/255 \n",
        "          labels_test_full = labels_test.values \n",
        "\n",
        "          # Concatenate training and test set\n",
        "          X_MNIST_full = np.concatenate((data_full, test_data_full))\n",
        "          y_MNIST_full = np.concatenate((labels_full, labels_test_full))\n",
        "\n",
        "          # Get rid of unused letters and the letters c, g, and h\n",
        "          X_MNIST = []\n",
        "          y_MNIST = []\n",
        "          for i in range(len(y_MNIST_full)):\n",
        "            if (y_MNIST_full[i] <= 8 and y_MNIST_full[i] != 2 and y_MNIST_full[i] != 6 and y_MNIST_full[i] != 7): #ADDED THE LAST AND STATEMENT TO GET RID OF I's\n",
        "              X_MNIST.append(X_MNIST_full[i])\n",
        "              y_MNIST.append(y_MNIST_full[i])\n",
        "          X_MNIST = np.asarray(X_MNIST)\n",
        "          y_MNIST = np.asarray(y_MNIST)\n",
        "\n",
        "          # Reshape data to 50x50\n",
        "          X_MNIST_50 = []\n",
        "          for i in range (len(X_MNIST)):\n",
        "            img_flat_orig = X_MNIST[i];\n",
        "            img_2d_orig = img_flat_orig.reshape(28, 28)\n",
        "            img_2d_new = cv2.resize(img_2d_orig, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
        "            img_1d_new = img_2d_new.flatten()\n",
        "            X_MNIST_50.append(img_1d_new)\n",
        "          X_MNIST_50 = np.asarray(X_MNIST_50)\n",
        "\n",
        "          idx = np.ones((9,1457))*-1 #this will hold the index values for each letter, will be -1 if rest of row is not full\n",
        "          for i in range(len(y_MNIST)):\n",
        "            curr_let = y_MNIST[i]\n",
        "            curr_row = idx[curr_let]\n",
        "            unique_elements, counts_elements = np.unique(curr_row, return_counts=True)\n",
        "            next_idx = len(unique_elements) - 1\n",
        "            idx[curr_let][next_idx] = i\n",
        "\n",
        "          X_MNIST_reduced = []\n",
        "          y_MNIST_reduced = []\n",
        "          for i in range(idx.shape[0]):\n",
        "            if (i == 2 or i == 6 or i == 7):\n",
        "              continue\n",
        "            for j in range(700): # <----------------------------- Change here to get reduced MNIST dataset size\n",
        "              ran_num = random.randint(0,idx.shape[1]-1)\n",
        "              while (idx[i][ran_num] == -1):\n",
        "                ran_num = random.randint(0,idx.shape[1]-1)\n",
        "              X_MNIST_reduced.append(X_MNIST_50[idx[i][ran_num].astype(int)])\n",
        "              y_MNIST_reduced.append(y_MNIST[idx[i][ran_num].astype(int)])\n",
        "          X_MNIST_reduced = np.asarray(X_MNIST_reduced)\n",
        "          y_MNIST_reduced = np.asarray(y_MNIST_reduced)\n",
        "\n",
        "          X_comb = np.concatenate((X, X_MNIST_reduced))\n",
        "          y_comb = np.concatenate((num_labels, y_MNIST_reduced))\n",
        "\n",
        "          X_2d = []\n",
        "          for i in X_comb:\n",
        "                X_2d.append(i.reshape(1, 50, 50))\n",
        "          X = np.array(X_2d)\n",
        "          num_labels = y_comb\n",
        "\n",
        "        X_data = []\n",
        "        for i in X:\n",
        "          X_data.append(i.reshape(1,50,50))\n",
        "        X = np.asarray(X_data)\n",
        "\n",
        "    \n",
        "        # Convert data & labels from numpy to PyTorch format\n",
        "        data = torch.FloatTensor(X)\n",
        "        #data = data.permute(0,3,1,2)        \n",
        "        labels = torch.LongTensor(num_labels)        \n",
        "\n",
        "        # Store data as part of the class        \n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.char_labels = char_labels\n",
        "        \n",
        "    def __len__(self):\n",
        "      \n",
        "        # Calculate and return the number of samples in the dataset\n",
        "        return(len(self.labels))\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "      \n",
        "        # Return the requested index of the data set as dictionary containing \n",
        "        # the data and the label\n",
        "        #sample = dict()\n",
        "        #sample['index'] = idx\n",
        "        #sample['data'] = self.data[idx]\n",
        "        #sample['label'] = self.labels[idx]\n",
        "        #sample['char_label'] = self.char_labels[idx]\n",
        "        \n",
        "        return self.data[idx], self.labels[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhVF6U2li6NA",
        "colab_type": "code",
        "outputId": "f5a002d8-10e6-49bc-d419-74ca388adaa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Initialize the dataset\n",
        "ASL_data = ASL_Dataset('drive/My Drive/train_data.npy', 'drive/My Drive/train_labels.npy',train=True)\n",
        "val_data = ASL_Dataset(\"drive/My Drive/ML Data/Test/Individual Testing/data.npy\", \"drive/My Drive/ML Data/Test/Individual Testing/labels.npy\",train=False)\n",
        "print(ASL_data.data.shape, val_data.data.shape)\n",
        "print(ASL_data.labels.shape, val_data.labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6044, 1, 50, 50]) torch.Size([5, 1, 50, 50])\n",
            "torch.Size([6044]) torch.Size([5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLJpOi-Cwfon",
        "colab_type": "code",
        "outputId": "3900c8e0-99ea-4ce3-e3ef-c216d7a1d8ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(np.max(ASL_data.labels.numpy()))\n",
        "print(np.max(val_data.labels.numpy()))\n",
        "print(ASL_data.labels.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "3\n",
            "[0 0 0 ... 8 8 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "921mDl_VldgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/justuser/sign-language-classifier-convnet-with-pytorch\n",
        " class Network(nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, 3)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(10, 15, 3) #15 or 20\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(15, 20, 3) #20 or 30\n",
        "        self.dropout1 = nn.Dropout2d()\n",
        "        \n",
        "        self.fc3 = nn.Linear(20 * 9 * 9, 270) \n",
        "        self.fc4 = nn.Linear(270, 9) \n",
        "        \n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "                \n",
        "        x = x.view(-1, 20 * 9 * 9) \n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        \n",
        "        return self.softmax(x)\n",
        "    \n",
        "    \n",
        "    def test(self, predictions, labels):\n",
        "        \n",
        "        self.eval()\n",
        "        correct = 0\n",
        "        for p, l in zip(predictions, labels):\n",
        "            if p == l:\n",
        "                correct += 1\n",
        "        \n",
        "        acc = correct / len(predictions)\n",
        "        print(\"Correct predictions: %5d / %5d (%5f)\" % (correct, len(predictions), acc))\n",
        "        \n",
        "    \n",
        "    def evaluate(self, predictions, labels):\n",
        "                \n",
        "        correct = 0\n",
        "        for p, l in zip(predictions, labels):\n",
        "            if p == l:\n",
        "                correct += 1\n",
        "        \n",
        "        acc = correct / len(predictions)\n",
        "        return(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1eYXu2LjRZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime \n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):  \n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader: \n",
        "            outputs = model(imgs)  \n",
        "            loss = loss_fn(outputs, labels.long()) \n",
        "\n",
        "            optimizer.zero_grad() \n",
        "            loss.backward() \n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item() \n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnFdcYZijKD3",
        "colab_type": "code",
        "outputId": "dda5c473-261c-4ebf-ad6e-1a3e03caa9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "train_loader = DataLoader(ASL_data, batch_size=200, shuffle=True)\n",
        "model = Network()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.08)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 60,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-19 04:58:18.138376 Epoch 1, Training loss 2.185305595397949\n",
            "2020-04-19 04:58:59.365117 Epoch 10, Training loss 0.7826145406692259\n",
            "2020-04-19 04:59:44.974264 Epoch 20, Training loss 0.3072689031400988\n",
            "2020-04-19 05:00:30.537557 Epoch 30, Training loss 0.17013832277828647\n",
            "2020-04-19 05:01:16.888558 Epoch 40, Training loss 0.17017340636061085\n",
            "2020-04-19 05:02:03.935358 Epoch 50, Training loss 0.0901885904491909\n",
            "2020-04-19 05:02:51.683026 Epoch 60, Training loss 0.10431538366021649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VZhDBzzrpwx",
        "colab_type": "code",
        "outputId": "d0f0f63b-d0ba-47e4-b610-c6f58035442c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(ASL_data, batch_size=200, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=True)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  \n",
        "            for imgs, labels in loader:\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.shape[0]  \n",
        "                correct += int((predicted == labels).sum()) \n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.96\n",
            "Accuracy val: 0.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HKBKTYHRgLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Individual Testing\n",
        "test1_img = np.load(\"drive/My Drive/ML Data/Test/Individual Testing/data.npy\")\n",
        "test1_labels = np.load(\"drive/My Drive/ML Data/Test/Individual Testing/labels.npy\")\n",
        "plt.imshow(test1_img[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YljHAX8VjFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_labels = np.squeeze(test1_labels)\n",
        "ulabels = np.unique(char_labels)\n",
        "num_labels = np.zeros(len(char_labels))\n",
        "for i in range(len(ulabels)):\n",
        "    num_labels[char_labels == ulabels[i]] = i\n",
        "X = []\n",
        "for i in range(test1_img.shape[0]):\n",
        "    img = test1_img[i]\n",
        "    img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
        "    img_g = rgb2gray(img_r)\n",
        "    img_1d = img_g.flatten()\n",
        "    img_f = img_1d.reshape(1,50,50)\n",
        "    X.append(img_f)\n",
        "X = np.asarray(X)              \n",
        "\n",
        "test_data = torch.FloatTensor(X)       \n",
        "test_labels = torch.LongTensor(num_labels) \n",
        "\n",
        "predictions = model(test_data)\n",
        "print(predictions)\n",
        "torch.max(predictions.data, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}