{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "sO9kjEdrijYM",
    "outputId": "8fa935f0-aa9e-45f3-984d-c5f310cf2fd3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from skimage.color import rgb2gray\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import datetime \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io \n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage import img_as_ubyte\n",
    "import os\n",
    "from skimage.util import random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zfWyMm64i6Kd"
   },
   "outputs": [],
   "source": [
    "class ASL_Dataset(Dataset):\n",
    "    def __init__(self, path_2_data, path_2_labels, train=None):\n",
    "      \n",
    "        # Load data from files\n",
    "        data = np.load(path_2_data)\n",
    "        char_labels = np.squeeze(np.load(path_2_labels))\n",
    "        \n",
    "        # Convert char labels to numeric\n",
    "        ulabels = np.unique(char_labels)\n",
    "        num_labels = np.zeros(len(char_labels))\n",
    "        for i in range(len(ulabels)):\n",
    "            num_labels[char_labels == ulabels[i]] = i\n",
    "\n",
    "        ########################################################\n",
    "        X = []\n",
    "        for i in range(data.shape[0]):\n",
    "            img = data[i]\n",
    "            img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "            img_g = rgb2gray(img_r)\n",
    "            img_1d = img_g.flatten()\n",
    "            #img_f = img_1d.reshape(1,50,50)\n",
    "            X.append(img_1d)\n",
    "        X = np.asarray(X)\n",
    "        ######################################################## \n",
    "        if (train == True):\n",
    "            data_raw = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "            test_data_raw = test = pd.read_csv(\"sign_mnist_test.csv\")\n",
    "\n",
    "            # Break up data and labels\n",
    "            labels = data_raw['label']\n",
    "            data_raw.drop('label', axis=1, inplace=True)\n",
    "            labels_test = test_data_raw['label']\n",
    "            test_data_raw.drop('label', axis=1, inplace=True)\n",
    "\n",
    "            # Normalize data\n",
    "            data_full = data_raw.values/255\n",
    "            labels_full = labels.values\n",
    "            test_data_full = test_data_raw.values/255 \n",
    "            labels_test_full = labels_test.values \n",
    "\n",
    "            # Concatenate training and test set\n",
    "            X_MNIST_full = np.concatenate((data_full, test_data_full))\n",
    "            y_MNIST_full = np.concatenate((labels_full, labels_test_full))\n",
    "\n",
    "            # Get rid of unused letters and the letters c, g, and h\n",
    "            X_MNIST = []\n",
    "            y_MNIST = []\n",
    "            for i in range(len(y_MNIST_full)):\n",
    "                if (y_MNIST_full[i] <= 8 and y_MNIST_full[i] != 2 and y_MNIST_full[i] != 6 and y_MNIST_full[i] != 7): #ADDED THE LAST AND STATEMENT TO GET RID OF I's\n",
    "                    X_MNIST.append(X_MNIST_full[i])\n",
    "                    y_MNIST.append(y_MNIST_full[i])\n",
    "            X_MNIST = np.asarray(X_MNIST)\n",
    "            y_MNIST = np.asarray(y_MNIST)\n",
    "\n",
    "          # Reshape data to 50x50\n",
    "            X_MNIST_50 = []\n",
    "            for i in range (len(X_MNIST)):\n",
    "                img_flat_orig = X_MNIST[i];\n",
    "                img_2d_orig = img_flat_orig.reshape(28, 28)\n",
    "                img_2d_new = cv2.resize(img_2d_orig, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "                img_1d_new = img_2d_new.flatten()\n",
    "                X_MNIST_50.append(img_1d_new)\n",
    "            X_MNIST_50 = np.asarray(X_MNIST_50)\n",
    "\n",
    "            idx = np.ones((9,1457))*-1 #this will hold the index values for each letter, will be -1 if rest of row is not full\n",
    "            for i in range(len(y_MNIST)):\n",
    "                curr_let = y_MNIST[i]\n",
    "                curr_row = idx[curr_let]\n",
    "                unique_elements, counts_elements = np.unique(curr_row, return_counts=True)\n",
    "                next_idx = len(unique_elements) - 1\n",
    "                idx[curr_let][next_idx] = i\n",
    "\n",
    "            X_MNIST_reduced = []\n",
    "            y_MNIST_reduced = []\n",
    "            for i in range(idx.shape[0]):\n",
    "                if (i == 2 or i == 6 or i == 7):\n",
    "                    continue\n",
    "            for j in range(700): # <----------------------------- Change here to get reduced MNIST dataset size\n",
    "                ran_num = random.randint(0,idx.shape[1]-1)\n",
    "                while (idx[i][ran_num] == -1):\n",
    "                    ran_num = random.randint(0,idx.shape[1]-1)\n",
    "                X_MNIST_reduced.append(X_MNIST_50[idx[i][ran_num].astype(int)])\n",
    "                y_MNIST_reduced.append(y_MNIST[idx[i][ran_num].astype(int)])\n",
    "            X_MNIST_reduced = np.asarray(X_MNIST_reduced)\n",
    "            y_MNIST_reduced = np.asarray(y_MNIST_reduced)\n",
    "\n",
    "            X_comb = np.concatenate((X, X_MNIST_reduced))\n",
    "            y_comb = np.concatenate((num_labels, y_MNIST_reduced))\n",
    "\n",
    "            X_2d = []\n",
    "            for i in X_comb:\n",
    "                  X_2d.append(i.reshape(1, 50, 50))\n",
    "            X = np.array(X_2d)\n",
    "            num_labels = y_comb\n",
    "\n",
    "        X_data = []\n",
    "        for i in X:\n",
    "            X_data.append(i.reshape(1,50,50))\n",
    "        X = np.asarray(X_data)\n",
    "\n",
    "    \n",
    "        # Convert data & labels from numpy to PyTorch format\n",
    "        data = torch.FloatTensor(X)\n",
    "        #data = data.permute(0,3,1,2)        \n",
    "        labels = torch.LongTensor(num_labels)        \n",
    "\n",
    "        # Store data as part of the class        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.char_labels = char_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "      \n",
    "        # Calculate and return the number of samples in the dataset\n",
    "        return(len(self.labels))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        # Return the requested index of the data set as dictionary containing \n",
    "        # the data and the label\n",
    "        #sample = dict()\n",
    "        #sample['index'] = idx\n",
    "        #sample['data'] = self.data[idx]\n",
    "        #sample['label'] = self.labels[idx]\n",
    "        #sample['char_label'] = self.char_labels[idx]\n",
    "        \n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IhVF6U2li6NA",
    "outputId": "f5a002d8-10e6-49bc-d419-74ca388adaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2544, 1, 50, 50]) torch.Size([709, 1, 50, 50])\n",
      "torch.Size([2544]) torch.Size([16993])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset\n",
    "ASL_data = ASL_Dataset('train_data.npy', 'train_labels.npy',train=True)\n",
    "val_data = ASL_Dataset(\"data.npy\", \"labels.npy\",train=False)\n",
    "print(ASL_data.data.shape, val_data.data.shape)\n",
    "print(ASL_data.labels.shape, val_data.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "xLJpOi-Cwfon",
    "outputId": "3900c8e0-99ea-4ce3-e3ef-c216d7a1d8ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "[0 0 0 ... 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "print(np.max(ASL_data.labels.numpy()))\n",
    "print(np.max(val_data.labels.numpy()))\n",
    "print(ASL_data.labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "921mDl_VldgI"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/justuser/sign-language-classifier-convnet-with-pytorch\n",
    "class Network(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10, 15, 3) #15 or 20\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(15, 20, 3) #20 or 30\n",
    "        self.dropout1 = nn.Dropout2d()\n",
    "        \n",
    "        self.fc3 = nn.Linear(20 * 9 * 9, 270) \n",
    "        self.fc4 = nn.Linear(270, 9) \n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "                \n",
    "        x = x.view(-1, 20 * 9 * 9) \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        return self.softmax(x)\n",
    "    \n",
    "    \n",
    "    def test(self, predictions, labels):\n",
    "        \n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        for p, l in zip(predictions, labels):\n",
    "            if p == l:\n",
    "                correct += 1\n",
    "        \n",
    "        acc = correct / len(predictions)\n",
    "        print(\"Correct predictions: %5d / %5d (%5f)\" % (correct, len(predictions), acc))\n",
    "        \n",
    "    \n",
    "    def evaluate(self, predictions, labels):\n",
    "                \n",
    "        correct = 0\n",
    "        for p, l in zip(predictions, labels):\n",
    "            if p == l:\n",
    "                correct += 1\n",
    "        \n",
    "        acc = correct / len(predictions)\n",
    "        return(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1eYXu2LjRZS"
   },
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):  \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader: \n",
    "            outputs = model(imgs)  \n",
    "            loss = loss_fn(outputs, labels.long()) \n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item() \n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "GnFdcYZijKD3",
    "outputId": "dda5c473-261c-4ebf-ad6e-1a3e03caa9bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-19 21:03:53.799724 Epoch 1, Training loss 2.1312367916107178\n",
      "2020-04-19 21:04:16.396042 Epoch 10, Training loss 1.8167612185845008\n",
      "2020-04-19 21:04:42.146597 Epoch 20, Training loss 1.4658675010387714\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(ASL_data, batch_size=200, shuffle=True)\n",
    "model = Network()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.08)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 60,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "7VZhDBzzrpwx",
    "outputId": "d0f0f63b-d0ba-47e4-b610-c6f58035442c"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(ASL_data, batch_size=200, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]  \n",
    "                correct += int((predicted == labels).sum()) \n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HKBKTYHRgLK"
   },
   "outputs": [],
   "source": [
    "# Individual Testing\n",
    "test1_img = np.load(\"data.npy\")\n",
    "test1_labels = np.load(\"labels.npy\")\n",
    "plt.imshow(test1_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YljHAX8VjFe"
   },
   "outputs": [],
   "source": [
    "char_labels = np.squeeze(test1_labels)\n",
    "ulabels = np.unique(char_labels)\n",
    "num_labels = np.zeros(len(char_labels))\n",
    "for i in range(len(ulabels)):\n",
    "    num_labels[char_labels == ulabels[i]] = i\n",
    "X = []\n",
    "for i in range(test1_img.shape[0]):\n",
    "    img = test1_img[i]\n",
    "    img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "    img_g = rgb2gray(img_r)\n",
    "    img_1d = img_g.flatten()\n",
    "    img_f = img_1d.reshape(1,50,50)\n",
    "    X.append(img_f)\n",
    "X = np.asarray(X)              \n",
    "\n",
    "test_data = torch.FloatTensor(X)       \n",
    "test_labels = torch.LongTensor(num_labels) \n",
    "\n",
    "predictions = model(test_data)\n",
    "print(predictions)\n",
    "torch.max(predictions.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DataLoader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
