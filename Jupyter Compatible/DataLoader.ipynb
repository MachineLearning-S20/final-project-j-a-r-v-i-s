{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "sO9kjEdrijYM",
    "outputId": "8fa935f0-aa9e-45f3-984d-c5f310cf2fd3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from skimage.color import rgb2gray\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import datetime \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io \n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage import img_as_ubyte\n",
    "import os\n",
    "from skimage.util import random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zfWyMm64i6Kd"
   },
   "outputs": [],
   "source": [
    "class ASL_Dataset(Dataset):\n",
    "    def __init__(self, path_2_data, path_2_labels, train=None):\n",
    "      \n",
    "        # Load data from files\n",
    "        data = np.load(path_2_data)\n",
    "        char_labels = np.squeeze(np.load(path_2_labels))\n",
    "        \n",
    "        # Convert char labels to numeric\n",
    "        ulabels = np.unique(char_labels)\n",
    "        num_labels = np.zeros(len(char_labels))\n",
    "        for i in range(len(ulabels)):\n",
    "            num_labels[char_labels == ulabels[i]] = i\n",
    "\n",
    "        ########################################################\n",
    "        X = []\n",
    "        for i in range(data.shape[0]):\n",
    "            img = data[i]\n",
    "            img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "            img_g = rgb2gray(img_r)\n",
    "            img_1d = img_g.flatten()\n",
    "            #img_f = img_1d.reshape(1,50,50)\n",
    "            X.append(img_1d)\n",
    "        X = np.asarray(X)\n",
    "        ######################################################## \n",
    "        if (train == True):\n",
    "            data_raw = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "            test_data_raw = test = pd.read_csv(\"sign_mnist_test.csv\")\n",
    "\n",
    "            # Break up data and labels\n",
    "            labels = data_raw['label']\n",
    "            data_raw.drop('label', axis=1, inplace=True)\n",
    "            labels_test = test_data_raw['label']\n",
    "            test_data_raw.drop('label', axis=1, inplace=True)\n",
    "\n",
    "            # Normalize data\n",
    "            data_full = data_raw.values/255\n",
    "            labels_full = labels.values\n",
    "            test_data_full = test_data_raw.values/255 \n",
    "            labels_test_full = labels_test.values \n",
    "\n",
    "            # Concatenate training and test set\n",
    "            X_MNIST_full = np.concatenate((data_full, test_data_full))\n",
    "            y_MNIST_full = np.concatenate((labels_full, labels_test_full))\n",
    "\n",
    "            # Get rid of unused letters and the letters c, g, and h\n",
    "            X_MNIST = []\n",
    "            y_MNIST = []\n",
    "            for i in range(len(y_MNIST_full)):\n",
    "                if (y_MNIST_full[i] <= 8 and y_MNIST_full[i] != 2 and y_MNIST_full[i] != 6 and y_MNIST_full[i] != 7): #ADDED THE LAST AND STATEMENT TO GET RID OF I's\n",
    "                    X_MNIST.append(X_MNIST_full[i])\n",
    "                    y_MNIST.append(y_MNIST_full[i])\n",
    "            X_MNIST = np.asarray(X_MNIST)\n",
    "            y_MNIST = np.asarray(y_MNIST)\n",
    "\n",
    "          # Reshape data to 50x50\n",
    "            X_MNIST_50 = []\n",
    "            for i in range (len(X_MNIST)):\n",
    "                img_flat_orig = X_MNIST[i];\n",
    "                img_2d_orig = img_flat_orig.reshape(28, 28)\n",
    "                img_2d_new = cv2.resize(img_2d_orig, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "                img_1d_new = img_2d_new.flatten()\n",
    "                X_MNIST_50.append(img_1d_new)\n",
    "            X_MNIST_50 = np.asarray(X_MNIST_50)\n",
    "\n",
    "            idx = np.ones((9,1457))*-1 #this will hold the index values for each letter, will be -1 if rest of row is not full\n",
    "            for i in range(len(y_MNIST)):\n",
    "                curr_let = y_MNIST[i]\n",
    "                curr_row = idx[curr_let]\n",
    "                unique_elements, counts_elements = np.unique(curr_row, return_counts=True)\n",
    "                next_idx = len(unique_elements) - 1\n",
    "                idx[curr_let][next_idx] = i\n",
    "\n",
    "            X_MNIST_reduced = []\n",
    "            y_MNIST_reduced = []\n",
    "            for i in range(idx.shape[0]):\n",
    "                if (i == 2 or i == 6 or i == 7):\n",
    "                    continue\n",
    "            for j in range(700): # <----------------------------- Change here to get reduced MNIST dataset size\n",
    "                ran_num = random.randint(0,idx.shape[1]-1)\n",
    "                while (idx[i][ran_num] == -1):\n",
    "                    ran_num = random.randint(0,idx.shape[1]-1)\n",
    "                X_MNIST_reduced.append(X_MNIST_50[idx[i][ran_num].astype(int)])\n",
    "                y_MNIST_reduced.append(y_MNIST[idx[i][ran_num].astype(int)])\n",
    "            X_MNIST_reduced = np.asarray(X_MNIST_reduced)\n",
    "            y_MNIST_reduced = np.asarray(y_MNIST_reduced)\n",
    "\n",
    "            X_comb = np.concatenate((X, X_MNIST_reduced))\n",
    "            y_comb = np.concatenate((num_labels, y_MNIST_reduced))\n",
    "\n",
    "            X_2d = []\n",
    "            for i in X_comb:\n",
    "                  X_2d.append(i.reshape(1, 50, 50))\n",
    "            X = np.array(X_2d)\n",
    "            num_labels = y_comb\n",
    "\n",
    "        X_data = []\n",
    "        for i in X:\n",
    "            X_data.append(i.reshape(1,50,50))\n",
    "        X = np.asarray(X_data)\n",
    "\n",
    "    \n",
    "        # Convert data & labels from numpy to PyTorch format\n",
    "        data = torch.FloatTensor(X)\n",
    "        #data = data.permute(0,3,1,2)        \n",
    "        labels = torch.LongTensor(num_labels)        \n",
    "\n",
    "        # Store data as part of the class        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.char_labels = char_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "      \n",
    "        # Calculate and return the number of samples in the dataset\n",
    "        return(len(self.labels))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        # Return the requested index of the data set as dictionary containing \n",
    "        # the data and the label\n",
    "        #sample = dict()\n",
    "        #sample['index'] = idx\n",
    "        #sample['data'] = self.data[idx]\n",
    "        #sample['label'] = self.labels[idx]\n",
    "        #sample['char_label'] = self.char_labels[idx]\n",
    "        \n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IhVF6U2li6NA",
    "outputId": "f5a002d8-10e6-49bc-d419-74ca388adaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2544, 1, 50, 50]) torch.Size([709, 1, 50, 50])\n",
      "torch.Size([2544]) torch.Size([16993])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset\n",
    "ASL_data = ASL_Dataset('train_data.npy', 'train_labels.npy',train=True)\n",
    "val_data = ASL_Dataset(\"data.npy\", \"labels.npy\",train=False)\n",
    "print(ASL_data.data.shape, val_data.data.shape)\n",
    "print(ASL_data.labels.shape, val_data.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "xLJpOi-Cwfon",
    "outputId": "3900c8e0-99ea-4ce3-e3ef-c216d7a1d8ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "[0 0 0 ... 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "print(np.max(ASL_data.labels.numpy()))\n",
    "print(np.max(val_data.labels.numpy()))\n",
    "print(ASL_data.labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "921mDl_VldgI"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/justuser/sign-language-classifier-convnet-with-pytorch\n",
    "class Network(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10, 15, 3) #15 or 20\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(15, 20, 3) #20 or 30\n",
    "        self.dropout1 = nn.Dropout2d()\n",
    "        \n",
    "        self.fc3 = nn.Linear(20 * 9 * 9, 270) \n",
    "        self.fc4 = nn.Linear(270, 9) \n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "                \n",
    "        x = x.view(-1, 20 * 9 * 9) \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        return self.softmax(x)\n",
    "    \n",
    "    \n",
    "    def test(self, predictions, labels):\n",
    "        \n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        for p, l in zip(predictions, labels):\n",
    "            if p == l:\n",
    "                correct += 1\n",
    "        \n",
    "        acc = correct / len(predictions)\n",
    "        print(\"Correct predictions: %5d / %5d (%5f)\" % (correct, len(predictions), acc))\n",
    "        \n",
    "    \n",
    "    def evaluate(self, predictions, labels):\n",
    "                \n",
    "        correct = 0\n",
    "        for p, l in zip(predictions, labels):\n",
    "            if p == l:\n",
    "                correct += 1\n",
    "        \n",
    "        acc = correct / len(predictions)\n",
    "        return(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1eYXu2LjRZS"
   },
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):  \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader: \n",
    "            outputs = model(imgs)  \n",
    "            loss = loss_fn(outputs, labels.long()) \n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item() \n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "GnFdcYZijKD3",
    "outputId": "dda5c473-261c-4ebf-ad6e-1a3e03caa9bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-19 21:03:53.799724 Epoch 1, Training loss 2.1312367916107178\n",
      "2020-04-19 21:04:16.396042 Epoch 10, Training loss 1.8167612185845008\n",
      "2020-04-19 21:04:42.146597 Epoch 20, Training loss 1.4658675010387714\n",
      "2020-04-19 21:05:08.385412 Epoch 30, Training loss 1.1077588292268605\n",
      "2020-04-19 21:05:33.782354 Epoch 40, Training loss 0.8619473714094895\n",
      "2020-04-19 21:05:59.384156 Epoch 50, Training loss 0.5750294809158032\n",
      "2020-04-19 21:06:25.657043 Epoch 60, Training loss 0.37952078535006595\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(ASL_data, batch_size=200, shuffle=True)\n",
    "model = Network()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.08)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 60,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "7VZhDBzzrpwx",
    "outputId": "d0f0f63b-d0ba-47e4-b610-c6f58035442c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.86\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 13699 is out of bounds for dimension 0 with size 709",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d5932497fea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy {}: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-d5932497fea8>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-27987d19faa5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m#sample['char_label'] = self.char_labels[idx]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 13699 is out of bounds for dimension 0 with size 709"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(ASL_data, batch_size=200, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]  \n",
    "                correct += int((predicted == labels).sum()) \n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HKBKTYHRgLK"
   },
   "outputs": [],
   "source": [
    "# Individual Testing\n",
    "test1_img = np.load(\"data.npy\")\n",
    "test1_labels = np.load(\"labels.npy\")\n",
    "plt.imshow(test1_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YljHAX8VjFe"
   },
   "outputs": [],
   "source": [
    "char_labels = np.squeeze(test1_labels)\n",
    "ulabels = np.unique(char_labels)\n",
    "num_labels = np.zeros(len(char_labels))\n",
    "for i in range(len(ulabels)):\n",
    "    num_labels[char_labels == ulabels[i]] = i\n",
    "X = []\n",
    "for i in range(test1_img.shape[0]):\n",
    "    img = test1_img[i]\n",
    "    img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "    img_g = rgb2gray(img_r)\n",
    "    img_1d = img_g.flatten()\n",
    "    img_f = img_1d.reshape(1,50,50)\n",
    "    X.append(img_f)\n",
    "X = np.asarray(X)              \n",
    "\n",
    "test_data = torch.FloatTensor(X)       \n",
    "test_labels = torch.LongTensor(num_labels) \n",
    "\n",
    "predictions = model(test_data)\n",
    "print(predictions)\n",
    "torch.max(predictions.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DataLoader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
