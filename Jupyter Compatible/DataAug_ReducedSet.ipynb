{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "0pWZtUDC_Un4",
    "outputId": "ea314385-c18e-4f58-de84-948e4aaaea66"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from skimage.color import rgb2gray\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import datetime \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io \n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from skimage import img_as_ubyte\n",
    "import os\n",
    "from skimage.util import random_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YpOozU_m_RDh"
   },
   "source": [
    "##Import/Reduce Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxGCImfLR2O0"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File drive/My Drive/sign_mnist_train.csv does not exist: 'drive/My Drive/sign_mnist_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efc4070ab071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/sign_mnist_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/My Drive/sign_mnist_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File drive/My Drive/sign_mnist_train.csv does not exist: 'drive/My Drive/sign_mnist_train.csv'"
     ]
    }
   ],
   "source": [
    "data_raw = pd.read_csv(\"drive/My Drive/sign_mnist_train.csv\")\n",
    "test_data_raw = test = pd.read_csv(\"drive/My Drive/sign_mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IQH7uffR8WM"
   },
   "outputs": [],
   "source": [
    "# Break up data and labels\n",
    "labels = data_raw['label']\n",
    "data_raw.drop('label', axis=1, inplace=True)\n",
    "labels_test = test_data_raw['label']\n",
    "test_data_raw.drop('label', axis=1, inplace=True)\n",
    "\n",
    "# Normalize data\n",
    "data_full = data_raw.values/255\n",
    "labels_full = labels.values\n",
    "test_data_full = test_data_raw.values/255 \n",
    "labels_test_full = labels_test.values \n",
    "\n",
    "# Concatenate training and test set\n",
    "X_MNIST_full = np.concatenate((data_full, test_data_full))\n",
    "y_MNIST_full = np.concatenate((labels_full,labels_test_full))\n",
    "\n",
    "# Get rid of unused letters and the letters c, g, and h\n",
    "X_MNIST = []\n",
    "y_MNIST = []\n",
    "for i in range(len(y_MNIST_full)):\n",
    "    if (y_MNIST_full[i] <= 8 and y_MNIST_full[i] != 2 and y_MNIST_full[i] != 6 and y_MNIST_full[i] != 7): #ADDED THE LAST AND STATEMENT TO GET RID OF I's\n",
    "        X_MNIST.append(X_MNIST_full[i])\n",
    "        y_MNIST.append(y_MNIST_full[i])\n",
    "X_MNIST = np.asarray(X_MNIST)\n",
    "y_MNIST = np.asarray(y_MNIST)\n",
    "\n",
    "# Reshape data to 50x50\n",
    "X_MNIST_50 = []\n",
    "for i in range (len(X_MNIST)):\n",
    "    img_flat_orig = X_MNIST[i];\n",
    "    img_2d_orig = img_flat_orig.reshape(28, 28)\n",
    "    img_2d_new = cv2.resize(img_2d_orig, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "    img_1d_new = img_2d_new.flatten()\n",
    "    X_MNIST_50.append(img_1d_new)\n",
    "X_MNIST_50 = np.asarray(X_MNIST_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sJ06n8JYR8Yg",
    "outputId": "eda019f9-6f2c-4633-81f3-bb1002f67038"
   },
   "outputs": [],
   "source": [
    "X_MNIST_50.shape, np.max(y_MNIST), np.min(y_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AUlMiH7e2sMR",
    "outputId": "fe7c4ebb-bdbe-4e5a-9137-727cc626d3ee"
   },
   "outputs": [],
   "source": [
    "letter_count = np.zeros(9)\n",
    "for i in range(len(y_MNIST)):\n",
    "    letter_count[y_MNIST[i]] += 1\n",
    "print(letter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "jwqGuutO3xJ9",
    "outputId": "d4d24d4b-b523-475f-c51c-6b4abe7da9c2"
   },
   "outputs": [],
   "source": [
    "# grab all the indexes for each letter\n",
    "\n",
    "idx = np.ones((9,1457))*-1 #this will hold the index values for each letter, will be -1 if rest of row is not full\n",
    "for i in range(len(y_MNIST)):\n",
    "    curr_let = y_MNIST[i]\n",
    "    curr_row = idx[curr_let]\n",
    "    unique_elements, counts_elements = np.unique(curr_row, return_counts=True)\n",
    "    next_idx = len(unique_elements) - 1\n",
    "    idx[curr_let][next_idx] = i\n",
    "    #print(len(unique_elements))\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bv50JKH36dDp"
   },
   "outputs": [],
   "source": [
    "# randomly choose 700 index for each letter to reduce the Kaggle dataset size to 50%\n",
    "X_MNIST_reduced = []\n",
    "y_MNIST_reduced = []\n",
    "for i in range(idx.shape[0]):\n",
    "    if (i == 2 or i == 6 or i == 7):\n",
    "        continue\n",
    "    for j in range(700): # <----------------------------- Change here to get reduced MNIST dataset size\n",
    "        ran_num = random.randint(0,idx.shape[1]-1)\n",
    "        while (idx[i][ran_num] == -1):\n",
    "            ran_num = random.randint(0,idx.shape[1]-1)\n",
    "        X_MNIST_reduced.append(X_MNIST_50[idx[i][ran_num].astype(int)])\n",
    "        y_MNIST_reduced.append(y_MNIST[idx[i][ran_num].astype(int)]+1)\n",
    "X_MNIST_reduced = np.asarray(X_MNIST_reduced)\n",
    "y_MNIST_reduced = np.asarray(y_MNIST_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fi3dvl3N92rj",
    "outputId": "7a01bd89-0abf-447d-e3a8-1ea811cd8a04"
   },
   "outputs": [],
   "source": [
    "X_MNIST_reduced.shape, y_MNIST_reduced.shape, np.max(y_MNIST_reduced), np.min(y_MNIST_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "0BF9Jyjv-MbK",
    "outputId": "41ff2b7d-13df-46cc-be12-21e49a8e77a3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(x = y_MNIST_reduced);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftfGZCrT_T9a"
   },
   "source": [
    "##Import/Data Augment Class Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NF-TDRC3STd6"
   },
   "outputs": [],
   "source": [
    "X_class_orig = np.load('drive/My Drive/train_data.npy')\n",
    "y_class_orig = np.load('drive/My Drive/train_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sXhRcWcDnFB9",
    "outputId": "e85d59c6-7de8-4758-8a34-0ea5c26a90f7"
   },
   "outputs": [],
   "source": [
    "y_class_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLzgxPUJSTeP"
   },
   "outputs": [],
   "source": [
    "# Change letters to numbers\n",
    "letters = {'A': 1, 'B': 2,'C': 3, 'D': 4,'E': 5, \n",
    "                        'F': 6,'G': 7, 'H': 8,'I': 9}\n",
    "y_class_or = []\n",
    "for i in range(len(y_class_orig)):\n",
    "    y_class_or.append(letters[y_class_orig[i][0]])\n",
    "y_class_or = np.asarray(y_class_or)\n",
    "\n",
    "# Normalize and format data\n",
    "X_class_or = []\n",
    "for i in range(X_class_orig.shape[0]):\n",
    "    img = X_class_orig[i]\n",
    "    img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "    img_g = rgb2gray(img_r)\n",
    "    img_1d = img_g.flatten()\n",
    "    X_class_or.append(img_1d)\n",
    "X_class_or = np.asarray(X_class_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z_mdTSLMBvzC",
    "outputId": "02046308-ca1b-46c1-a0f0-c79cefda1085"
   },
   "outputs": [],
   "source": [
    "X_class_or.shape, y_class_or.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W-JJ_4KOCdDw",
    "outputId": "269c8910-2024-46e8-dd70-5bf02000ef41"
   },
   "outputs": [],
   "source": [
    "# take some original class images for testing purposes\n",
    "X_train_or, X_test_or, y_train_or, y_test_or = train_test_split(X_class_or, y_class_or,\n",
    "                                                                 test_size=0.4, random_state=5)\n",
    "X_train_or.shape, X_test_or.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "2gtI7MT1AoRB",
    "outputId": "c14dec07-ea74-493c-b7b4-aab71c64f65d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(x = y_train_or);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myZivVmlA898"
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/image-augmentation-using-python-numpy-opencv-and-skimage-ef027e9898da\n",
    "def anticlockwise_rotation(image):\n",
    "    angle= random.randint(0,8)\n",
    "    return rotate(image, angle)\n",
    "\n",
    "def clockwise_rotation(image):\n",
    "    angle= random.randint(0,8)\n",
    "    return rotate(image, -angle)\n",
    "\n",
    "def h_flip(image):\n",
    "    return  np.fliplr(image)\n",
    "\n",
    "#def v_flip(image):\n",
    "    #return np.flipud(image)\n",
    "\n",
    "def add_noise(image):\n",
    "    return random_noise(image)\n",
    "\n",
    "def blur_image(image):\n",
    "    return cv2.GaussianBlur(img, (9,9),0)\n",
    "\n",
    "def warp_shift(image): \n",
    "    transform = AffineTransform(translation=(0,40)) \n",
    "    warp_image = warp(image, transform, mode=\"wrap\")\n",
    "    return warp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "auRqIPokBE1i"
   },
   "outputs": [],
   "source": [
    "def Data_Aug(X, y, number_imgs):\n",
    "    transformations = {'rotate anticlockwise': anticlockwise_rotation,\n",
    "                        'rotate clockwise': clockwise_rotation,\n",
    "                        'horizontal flip': h_flip, \n",
    "                        #'vertical flip': v_flip,\n",
    "                    'warp shift': warp_shift,\n",
    "                    'adding noise': add_noise}\n",
    "\n",
    "    X_new_class = []\n",
    "    y_new_class = []\n",
    "    X_using = X\n",
    "    y_using = y\n",
    "\n",
    "  # Lets data augment class data by 500 each, or 500*9 = 4500\n",
    "\n",
    "    images_to_generate = number_imgs\n",
    "    i = 1\n",
    "    while i <= images_to_generate:\n",
    "        img_idx = random.randint(0, len(X_using)-1)\n",
    "        y_new_class.append(y_using[img_idx])\n",
    "        original_image = X_using[img_idx].reshape(50,50)\n",
    "        transformed_image=None\n",
    "        n = 0  \n",
    "        transformation_count = random.randint(1, len(transformations))\n",
    "        while n <= transformation_count:\n",
    "            key = random.choice(list(transformations)) #randomly choosing method to call\n",
    "            transformed_image = transformations[key](original_image)\n",
    "            n = n + 1\n",
    "        trans_img = transformed_image.flatten()\n",
    "        X_new_class.append(trans_img);\n",
    "        i += 1\n",
    "    X_new_class = np.asarray(X_new_class)\n",
    "    y_new_class = np.asarray(y_new_class)\n",
    "    return X_new_class, y_new_class\n",
    "#X_new_class.shape, y_new_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ioF0TBlp1QR"
   },
   "outputs": [],
   "source": [
    "X_new_class, y_new_class = Data_Aug(X_train_or, y_train_or,4500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "Re4fj3XhCEMI",
    "outputId": "abf4278e-4c5a-4b4d-c6d4-a2e55e82da1c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(x = y_new_class);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "colab_type": "code",
    "id": "3cT3wNgpCJ2V",
    "outputId": "9b8f0f9f-a0a5-4e8f-faaf-160afcbfa7fd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for i in range(1,16):\n",
    "    idx = random.randint(0, len(X_new_class))\n",
    "    img = X_new_class[idx]\n",
    "    plt.subplot(3,5,i)\n",
    "    plt.title(y_new_class[idx]+1)\n",
    "    plt.imshow(img.reshape(50,50), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxDVDrTtAkCf"
   },
   "source": [
    "###Add C Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ql8QC_aSTeA"
   },
   "outputs": [],
   "source": [
    "C_data = np.load('drive/My Drive/C/data.npy')\n",
    "C_labels = np.load('drive/My Drive/C/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQLTdJ55STeD"
   },
   "outputs": [],
   "source": [
    "C_labels.resize(709,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0cjBxGjSTeG"
   },
   "outputs": [],
   "source": [
    "#X_class_raw_C = np.concatenate((X_class_orig, C_data))\n",
    "#y_class_raw_C = np.concatenate((y_class_orig, C_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlCJb0W6STeL"
   },
   "outputs": [],
   "source": [
    "# Change letters to numbers\n",
    "letters = {'A': 1, 'B': 2,'C': 3, 'D': 4,'E': 5, \n",
    "                        'F': 6,'G': 7, 'H': 8,'I': 9}\n",
    "y_C = []\n",
    "for i in range(len(C_labels)):\n",
    "    y_C.append(letters[C_labels[i][0]])\n",
    "y_C = np.asarray(y_C)\n",
    "\n",
    "# Normalize and format data\n",
    "X_C = []\n",
    "for i in range(C_data.shape[0]):\n",
    "    img = C_data[i]\n",
    "    img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "    img_g = rgb2gray(img_r)\n",
    "    img_1d = img_g.flatten()\n",
    "    X_C.append(img_1d)\n",
    "X_C = np.asarray(X_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "5o-EMcXZ_2kT",
    "outputId": "9cc05b8a-4898-4b0f-e809-f45555c29722"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(x = y_C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYv8wQ8BN8I5"
   },
   "source": [
    "##Combine Kaggle and Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpHAmxK2OYUS"
   },
   "source": [
    "###Without Extra C Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cxHCGkr6N-RG",
    "outputId": "e72ac132-dfc3-4508-a32e-b51cc78acc43"
   },
   "outputs": [],
   "source": [
    "X_train_comb_noC = np.concatenate((X_MNIST_reduced, X_train_or, X_new_class))\n",
    "y_train_comb_noC = np.concatenate((y_MNIST_reduced, y_train_or, y_new_class))\n",
    "X_train_comb_noC.shape, y_train_comb_noC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "sZzqW3EaN-UF",
    "outputId": "100b3053-d41b-428f-b9c1-74cdde9871a1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(x = y_train_comb_noC);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGlZaVljOa6X"
   },
   "source": [
    "###With Extra C Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0wNCg99DOmUj",
    "outputId": "d5c8b89d-c7b9-4c81-caf0-b880f2f6cfd2"
   },
   "outputs": [],
   "source": [
    "X_train_comb_C = np.concatenate((X_MNIST_reduced, X_train_or, X_new_class, X_C))\n",
    "y_train_comb_C = np.concatenate((y_MNIST_reduced, y_train_or, y_new_class, y_C))\n",
    "X_train_comb_C.shape, y_train_comb_C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "dES21Qa7OmXb",
    "outputId": "73382d4b-bae7-4965-de3c-4f53c8bea742"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(x = y_train_comb_C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7wcV4xfqsSK"
   },
   "source": [
    "###Data Augmentation on Class and MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aSKyzNM5qwTm"
   },
   "outputs": [],
   "source": [
    "X_train_comb = np.concatenate((X_MNIST_reduced, X_train_or, X_C))\n",
    "y_train_comb = np.concatenate((y_MNIST_reduced, y_train_or, y_C))\n",
    "X_train_comb_new, y_train_comb_new = Data_Aug(X_train_comb, y_train_comb, 4000)\n",
    "X_train_full_DA = np.concatenate((X_train_comb, X_train_comb_new))\n",
    "y_train_full_DA = np.concatenate((y_train_comb, y_train_comb_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "_UaHgmM8qwWm",
    "outputId": "e6140a5c-e65d-4499-c93d-8afe9c828018"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(x = y_train_full_DA);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUgds_Q7DQlf"
   },
   "source": [
    "##Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BW86MGasD5j-"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/justuser/sign-language-classifier-convnet-with-pytorch\n",
    " class Network(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(10, 15, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(15, 20, 3) \n",
    "        self.dropout1 = nn.Dropout2d()\n",
    "        \n",
    "        self.fc3 = nn.Linear(20 * 9 * 9, 270) \n",
    "        self.fc4 = nn.Linear(270, 9) \n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "                \n",
    "        x = x.view(-1, 20 * 9 * 9) \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        return self.softmax(x)\n",
    "    \n",
    "    \n",
    "    def test(self, predictions, labels):\n",
    "        \n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        for p, l in zip(predictions, labels):\n",
    "            if p == l:\n",
    "                correct += 1\n",
    "        \n",
    "        acc = correct / len(predictions)\n",
    "        print(\"Correct predictions: %5d / %5d (%5f)\" % (correct, len(predictions), acc))\n",
    "        \n",
    "    \n",
    "    def evaluate(self, predictions, labels):\n",
    "                \n",
    "        correct = 0\n",
    "        for p, l in zip(predictions, labels):\n",
    "            if p == l:\n",
    "                correct += 1\n",
    "        \n",
    "        acc = correct / len(predictions)\n",
    "        return(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GRLQuVlbDS2g",
    "outputId": "5cc24899-f009-494c-c309-737210aee47e"
   },
   "outputs": [],
   "source": [
    "# function takes in incorrect format of training data, NEED TO FIX\n",
    "#def train(X_train, Y_train):\n",
    "X_train = X_train_comb_C\n",
    "Y_train = y_train_comb_C\n",
    "X_2d = []\n",
    "for i in X_train:\n",
    "    X_2d.append(i.reshape(1, 50, 50))\n",
    "X_2d = np.array(X_2d)\n",
    "\n",
    "X_train_set, X_val_set, y_train_set, y_val_set = train_test_split(X_2d, Y_train, test_size=0.2, random_state=4)\n",
    "\n",
    "X = torch.FloatTensor(X_train_set)\n",
    "y = torch.LongTensor(y_train_set.tolist())-1\n",
    "\n",
    "X_test = torch.FloatTensor(X_val_set)\n",
    "y_test = torch.LongTensor(y_val_set.tolist())-1\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 80\n",
    "learning_rate = 0.01\n",
    "\n",
    "net = Network()\n",
    "#optimizer = optim.Adam(net.parameters(),lr=learning_rate)\n",
    "optimizer = optim.SGD(net.parameters(), learning_rate, momentum=0.7)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "net.train()\n",
    "loss_log = []\n",
    "acc_log = []\n",
    "for e in range(epochs):\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        X_batch = X[i:i + batch_size] \n",
    "        y_batch = y[i:i + batch_size] \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(Variable(X_batch))\n",
    "\n",
    "        loss = loss_func(net_out, Variable(y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "          #pred = net(Variable(test_data_formated))\n",
    "            loss_log.append(loss.item())\n",
    "            acc_log.append(net.evaluate(torch.max(net(Variable(X_test[:500])).data, 1)[1], y_test[:500]))\n",
    "\n",
    "    print('Epoch: {} - Loss: {:.6f}'.format(e + 1, loss.item()))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(loss_log[2:])\n",
    "plt.plot(acc_log)\n",
    "plt.plot(np.ones(len(acc_log)), linestyle='dashed')\n",
    "\n",
    "  #return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azK3gVrXNyc8"
   },
   "source": [
    "##Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxGuYnAcJ5Ey"
   },
   "outputs": [],
   "source": [
    "def convert_2d(data, dim):\n",
    "    data_new = []\n",
    "    for i in data:\n",
    "        data_new.append(i.reshape(1, dim, dim))\n",
    "    return np.array(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gfgVaDjJJq2s",
    "outputId": "1ef9044f-9d14-40a6-83fb-fad80656dbc8"
   },
   "outputs": [],
   "source": [
    "# Testing against class dataset\n",
    "possibleLabels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]\n",
    "\n",
    "net.eval()\n",
    "X_test_2d = convert_2d(X_test_or, 50)\n",
    "X_class_test = torch.FloatTensor(X_test_2d)\n",
    "y_class_test = torch.LongTensor(y_test_or.tolist())-1\n",
    "\n",
    "predictions = net(Variable(X_class_test))\n",
    "net.test(torch.max(predictions.data, 1)[1], y_class_test)\n",
    "performMetrics(predictions.data, y_class_test, possibleLabels, \"CNN for Class test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HKBKTYHRgLK"
   },
   "outputs": [],
   "source": [
    "# Individual Testing\n",
    "test1_img = np.load(\"drive/My Drive/ML Data/Test/Individual Testing/data.npy\")\n",
    "test1_label = np.load(\"drive/My Drive/ML Data/Test/Individual Testing/labels.npy\")\n",
    "plt.imshow(test1_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQ1i70DXJopJ"
   },
   "outputs": [],
   "source": [
    "def convert2DTo1D(arr):\n",
    "    if (len(arr) == 0):\n",
    "        return;\n",
    "    temp = np.array(arr[0]);\n",
    "    for i in range(1,len(arr)):\n",
    "        temp = np.append(temp, arr[i]);\n",
    "    return temp\n",
    "\n",
    "#Convert all the photos into 1D arrays\n",
    "def convertAllPhotosTo1DArrays(threeDArr):\n",
    "    if (len(threeDArr.shape) < 2):\n",
    "        print(\"Array size is not 3D\")\n",
    "        return threeDArr;\n",
    "    twoDArr = np.zeros(shape=(len(threeDArr),10000))\n",
    "    for i in range(len(threeDArr)):\n",
    "        twoDArr[i] = convert2DTo1D(threeDArr[i])\n",
    "    return twoDArr\n",
    "\n",
    "#Calculates the accuracy for each class\n",
    "def createAccuracyPerClassArray(y_actual, y_predict, labels):\n",
    "    correct = [0] * len(labels)\n",
    "    total = [0] * len(labels)\n",
    "    for i in range(len(y_actual)):\n",
    "        if y_actual[i] == y_predict[i]:\n",
    "            correct[labels.index(y_actual[i])] += 1\n",
    "        total[labels.index(y_actual[i])] += 1\n",
    "    accuracy = [0] * len(labels)\n",
    "    for i in range(len(correct)):\n",
    "        if (total[i] == 0):\n",
    "            accuracy[i] = 0\n",
    "        else:\n",
    "            accuracy[i] = correct[i] / total[i]\n",
    "    return accuracy\n",
    "\n",
    "def makeAccuracyGraph(arr, training, startingNum=0, title=\"\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(startingNum, startingNum + len(arr)),arr,color='blue', linestyle='dashed', marker=\"o\",\n",
    "            markerfacecolor='red', markersize=10, label=\"Test\")\n",
    "    plt.plot(range(startingNum, startingNum + len(arr)),arr,color='green', linestyle='-.', marker=\"o\",\n",
    "            markerfacecolor='purple', markersize=10, label=\"Training\")\n",
    "    plt.title(\"Accuracy vs\" + title)\n",
    "    plt.xlabel(title)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#Prints the perclass accuracy\n",
    "def printPerClassAccuracies(accuracy, labels, title=\"\"):\n",
    "      for i in range(len(accuracy)):\n",
    "        print(title, \" \", labels[i], \" accuracy: \", accuracy[i])\n",
    "    \n",
    "#Prints the overall accuracy and the per class accuracy for each N \n",
    "#(for example, if I were testing K different neighbors for KNN, the arr would be len(K), the perClass would be a 2D array with [K, len(labels)])\n",
    "def printAccuracies(arr, training, perClass, labels, startingNum = 0, title=\"\"):\n",
    "    for i in range(len(arr)):\n",
    "        print(title, \" - \", startingNum+i, \" overall accuracy: \", arr[i])\n",
    "        print(\"Per class accuracies for \", title, startingNum+i)\n",
    "        printPerClassAccuracies(perClass[i],labels)\n",
    "\n",
    "    makeAccuracyGraph(arr, training, startingNum, title)\n",
    "\n",
    "#Metrics function\n",
    "def performMetrics(y_te, y_pre, labels, name=\"\"):\n",
    "    y_test = [-1] * y_te.shape[0] #Create a N sized array, where N is the size of the training set\n",
    "    i_max = -1000\n",
    "    for i in range(len(y_te)):\n",
    "        i_max = -1\n",
    "        for j in range(len(y_te[i])): \n",
    "            if y_te[i][j] > i_max:\n",
    "                i_max = y_te[i][j]\n",
    "                y_test[i] = j\n",
    "      #JD6 We could easily just test here that if it is less than a threshold leave it at -1\n",
    "      #y_test[i] = j #if i_max < threshold\n",
    "\n",
    "    y_predict = y_pre.tolist()\n",
    "\n",
    "    #Converting them to be the letters\n",
    "    for i in range(len(y_predict)):\n",
    "        y_test[i] = labels[y_test[i]]\n",
    "        y_predict[i] = labels[y_predict[i]]\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(y_test)):\n",
    "        count += 1 if (y_test[i] == y_predict[i]) else 0\n",
    "    print(name, \" Overall average: \", (count/len(y_test)))\n",
    "\n",
    "    perClass = createAccuracyPerClassArray(y_test,y_predict,labels)\n",
    "    printPerClassAccuracies(perClass, labels, name)\n",
    "\n",
    "    #Confusion Maxtrix - Lecture 10\n",
    "    C = metrics.confusion_matrix(y_test,y_predict,labels=labels)\n",
    "    print(C)\n",
    "    C_scaled = list()\n",
    "    #Scaling each value\n",
    "    #JD6, may have to put an array of possible labels here?\n",
    "    for row in range(0,len(C)):\n",
    "        total = 0\n",
    "        C_scaled.append(list())\n",
    "        for col in range(0,len(C[row])):\n",
    "            total += C[row][col] \n",
    "        for col in range(0,len(C[row])):\n",
    "            C_scaled [row].append(C[row][col] / total) \n",
    "            #Divide each one to get the %\n",
    "\n",
    "    plt.figure(figsize = (15,15))\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(C_scaled, annot=True)\n",
    "\n",
    "    ax.set_title(\"Confusion matrix -\" + name)\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    plt.show()\n",
    "\n",
    "    #Taken from https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    lw = 2\n",
    "    n_classes = len(labels)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        #From Dr. Silvia's OH\n",
    "        y_t = [True] * len(y_test)\n",
    "        y_p = [True] * len(y_test)\n",
    "        for j in range(len(y_test)):\n",
    "            y_t[j] = 1 if (y_test[j] == labels[i]) else 0\n",
    "            y_p[j] = 1 if (y_predict[j] == labels[i]) else 0\n",
    "        fpr[labels[i]], tpr[labels[i]], _ = metrics.roc_curve(y_t, y_p)\n",
    "        roc_auc[labels[i]] = metrics.auc(fpr[labels[i]], tpr[labels[i]])\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[labels[i]] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[labels[i]], tpr[labels[i]])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = metrics.auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "          label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                ''.format(roc_auc[\"macro\"]),\n",
    "          color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', \"y\", \"k\", \"g\", \"r\", \"m\", \"b\", \"c\"])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[labels[i]], tpr[labels[i]], color=color, lw=lw,\n",
    "              label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "              ''.format(labels[i], roc_auc[labels[i]]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve for each digit')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFn1O5250FZK"
   },
   "source": [
    "##Load Previous Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "QOlxF8IVvyPf",
    "outputId": "c2617849-2352-45e8-c998-a9733c3bde2e"
   },
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.load_state_dict(torch.load('drive/My Drive/model_wFullDataAug.pth'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NPvwNNON0Ift",
    "outputId": "7f2f28fb-f778-413e-ac13-4c7acb72ce98"
   },
   "outputs": [],
   "source": [
    "X_train_or, X_test_or, y_train_or, y_test_or = train_test_split(X_class_or, y_class_or,\n",
    "                                                                 test_size=0.1, random_state=800)\n",
    "X_test_or.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AzjtWnIJ0tLs",
    "outputId": "d642b19f-fe6a-49ed-d680-8b5a2f9c2209"
   },
   "outputs": [],
   "source": [
    "X_test_new, y_test_new = Data_Aug(X_class_or,y_class_or,300)\n",
    "X_test_new.shape, y_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VaX7U81Tv3_E",
    "outputId": "163d00d5-a41d-4f96-9132-0d3042c72e94"
   },
   "outputs": [],
   "source": [
    "possibleLabels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]\n",
    "X_test_2d = convert_2d(X_test_or, 50)\n",
    "X_class_test = torch.FloatTensor(X_test_2d)\n",
    "y_class_test = torch.LongTensor(y_test_or.tolist())\n",
    "\n",
    "predictions = net(Variable(X_class_test))\n",
    "net.test(torch.max(predictions.data, 1)[1], y_class_test)\n",
    "performMetrics(predictions.data, y_class_test, possibleLabels, \"CNN for Class test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmmvQUzf0Umt"
   },
   "outputs": [],
   "source": [
    "def test(X, y):\n",
    "  # Normalize and format data\n",
    "    X_new = []\n",
    "    for i in range(X.shape[0]):\n",
    "        img = X[i]\n",
    "        img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "        img_g = rgb2gray(img_r)\n",
    "        img_f = img_g.reshape(1, 50, 50)\n",
    "    #img_1d = img_g.flatten()\n",
    "        X_new.append(img_f)\n",
    "    X_new = np.asarray(X_new)\n",
    "\n",
    "    net = Network();\n",
    "    net.load_state_dict(torch.load('drive/My Drive/model_wFullDataAug.pth'));\n",
    "    net.eval()\n",
    "\n",
    "    X_test = torch.FloatTensor(X_new)\n",
    "    y_test = torch.LongTensor(y.tolist())\n",
    "    predictions = net(Variable(X_test))\n",
    "    net.test(torch.max(predictions.data, 1)[1], y_test)\n",
    "    performMetrics(predictions.data, y_test, possibleLabels, \"CNN for Class test set\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3khmX_fj7WXg"
   },
   "outputs": [],
   "source": [
    "X_test = np.load('drive/My Drive/ML Data/Test/data.npy')\n",
    "y_test = np.load('drive/My Drive/ML Data/Test/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "mW-ntWqM-jwi",
    "outputId": "dc33055b-02b0-4a83-d47c-94d60699a591"
   },
   "outputs": [],
   "source": [
    "img0 = X_test[0]\n",
    "plt.imshow(img0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myoBEAHiIAhH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BPeJuq78l40"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "letters = {'A': 0, 'B': 1,'C': 2, 'D': 3,'E': 4, \n",
    "                        'F': 5,'G': 6, 'H': 7,'I': 8}\n",
    "y_new = []\n",
    "for i in range(len(y_test)):\n",
    "    y_new.append(letters[y_test[i][0]])\n",
    "y_new = np.asarray(y_new)\n",
    "\n",
    "X_new = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    img = X_test[i]\n",
    "    img_r = cv2.resize(img, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "    img_g = rgb2gray(img_r)\n",
    "    img_1d = img_g.flatten()\n",
    "    X_new.append(img_1d)\n",
    "X_new = np.asarray(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "da7_KGdTFxy-",
    "outputId": "a8de498d-c1b0-4a61-e5e7-5c3d8a41b5d1"
   },
   "outputs": [],
   "source": [
    "def convert_2d(data, dim):\n",
    "    data_new = []\n",
    "    for i in data:\n",
    "        data_new.append(i.reshape(1, dim, dim))\n",
    "    return np.array(data_new)\n",
    "\n",
    "num = 29\n",
    "X_img = X_new[num]\n",
    "X_1 = X_img.reshape(1,len(X_img))\n",
    "plt.imshow(X_img.reshape(50,50),cmap='gray');plt.show();\n",
    "y_img = y_new[num]\n",
    "print(\"Correct label: \",y_img)\n",
    "\n",
    "\n",
    "X_test_2d = convert_2d(X_1, 50)\n",
    "X = torch.FloatTensor(X_test_2d)\n",
    "y = torch.LongTensor(y_img.tolist())\n",
    "predictions = net(Variable(X))\n",
    "y_te = predictions.data\n",
    "y_test1 = [-1] * y_te.shape[0] #Create a N sized array, where N is the size of the training set\n",
    "i_max = -1000\n",
    "for i in range(len(y_te)):\n",
    "    i_max = -1\n",
    "    for j in range(len(y_te[i])): \n",
    "        if y_te[i][j] > i_max:\n",
    "            i_max = y_te[i][j]\n",
    "            y_test1[i] = j\n",
    "print(\"Predicted Label: \",y_test1[0])\n",
    "net.test(torch.max(predictions.data, 1)[1], y)\n",
    "#performMetrics(predictions.data, y, possibleLabels, \"CNN for Class test set\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DataAug_ReducedSet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
